{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Byes Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(word,list):\n",
    "    c=0\n",
    "    for i in list:\n",
    "        if(i==word):\n",
    "            c+=1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text=[]):\n",
    "    stop_words=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"you're\", \"you've\", \"you'll\", \"you'd\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"she's\", \"her\", \"hers\", \"herself\", \"it\", \"it's\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"that'll\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"don't\", \"should\", \"should've\", \"now\", \"d\", \"ll\", \"m\", \"o\", \"re\", \"ve\", \"y\", \"ain\", \"aren\", \"aren't\", \"couldn\", \"couldn't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", \"hadn\", \"hadn't\", \"hasn\", \"hasn't\", \"haven\", \"haven't\", \"isn\", \"isn't\", \"ma\", \"mightn\", \"mightn't\", \"mustn\", \"mustn't\", \"needn\", \"needn't\", \"shan\", \"shan't\", \"shouldn\", \"shouldn't\", \"wasn\", \"wasn't\", \"weren\", \"weren't\", \"won\", \"won't\", \"wouldn\", \"wouldn't\"]\n",
    "    word=\"\"\n",
    "    text+=\"\"\n",
    "    l=len(text)\n",
    "    clean_text_list=[]\n",
    "    for i in range(l):\n",
    "        if(text[i]==\" \" or i==(l-1)):\n",
    "            if(i==(l-1)):\n",
    "                word+=text[i]\n",
    "            f=count(word.lower(),stop_words)\n",
    "            if(f==0):\n",
    "                clean_text_list.append(word)\n",
    "            word=\"\"\n",
    "            continue\n",
    "        word+=text[i]\n",
    "    return clean_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(email):\n",
    "    email=np.array(email)\n",
    "    feature_list=[]\n",
    "    frequency_list=[]\n",
    "    l=len(email)\n",
    "    for i in range(l):\n",
    "        text=email[i]\n",
    "        text=str(text)\n",
    "        word_list=remove_stop_words(text)\n",
    "        words,frequency=np.unique(word_list,return_counts=True)\n",
    "        l1=len(frequency)\n",
    "        for j in range(l1):\n",
    "            if(frequency[j]>6):\n",
    "                feature_list.append(words[j])\n",
    "    feature_list=list(dict.fromkeys(feature_list))\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(email,feature_names):\n",
    "    email=np.array(email)\n",
    "    data=[]\n",
    "    l=len(email)\n",
    "    for i in range(l):\n",
    "        d=[]\n",
    "        text=email[i]\n",
    "        text=str(text)\n",
    "        word_list=remove_stop_words(text)\n",
    "        for feature in feature_names:\n",
    "            frequency=count(feature,word_list)\n",
    "            d.append(frequency)\n",
    "        data.append(d)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dictionary(data,y_train):\n",
    "    result={}\n",
    "    classes=set(y_train)\n",
    "  #  data=np.array(data)\n",
    "    y_train=np.array(y_train)\n",
    "    for current_class in classes:\n",
    "        result[current_class]={}\n",
    "        result[\"total_data\"]=len(y_train)\n",
    "        current_class_rows=(y_train==current_class)\n",
    "        x_train_current=data.iloc[current_class_rows,:]\n",
    "        y_train_current=y_train[current_class_rows]\n",
    "        l=data.shape[1]-1\n",
    "        features=data.columns\n",
    "        for i in range(l):\n",
    "            result[current_class][features[i].lower()]=x_train_current.iloc[:,i].sum()\n",
    "        result[current_class][\"total_count\"]=len(y_train_current)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train,y_train):\n",
    "    key_words=generate_features(x_train)\n",
    "    key_word_frequency=generate_data(x_train,key_words)\n",
    "    data=pd.DataFrame(key_word_frequency,columns=key_words)\n",
    "    dictionary=generate_dictionary(data,y_train)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(current_class,x,dictionary={}):\n",
    "    x=str(x)\n",
    "    output=np.log(dictionary[current_class][\"total_count\"])-np.log(dictionary[\"total_data\"])\n",
    "    word_list=remove_stop_words(x)\n",
    "    features=dictionary[current_class].keys()\n",
    "    for word in word_list:\n",
    "        word=word.lower()\n",
    "        word_count=count(word,features)\n",
    "        if(word_count>0):\n",
    "            l=len(features)\n",
    "            #for i in range(l):\n",
    "         #       if(features[i]==word):\n",
    "          #          break\n",
    "            count_current_class_with_word=dictionary[current_class][word]+1\n",
    "            count_current_class=dictionary[current_class][\"total_count\"]+len(dictionary[current_class].keys())-1\n",
    "            current_probaility=word_count*(np.log(count_current_class_with_word)-np.log(count_current_class))\n",
    "            output=output+current_probaility\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_point_predict(x,dictionary):\n",
    "    x=np.array(x)\n",
    "    classes=dictionary.keys()\n",
    "    best_c=-1\n",
    "    best_p=-1\n",
    "    first_run=True\n",
    "    for current_class in classes:\n",
    "        if(current_class==\"total_data\"):\n",
    "            continue\n",
    "        current_prob=probability(current_class,x,dictionary)\n",
    "        if(first_run==True or best_p<current_prob):\n",
    "            best_p=current_prob\n",
    "            best_c=current_class\n",
    "        first_run=False\n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_test,dictionary):\n",
    "    x_test=np.array(x_test)\n",
    "    y_pred=[]\n",
    "    for x in x_test:\n",
    "        pred=single_point_predict(x,dictionary)\n",
    "        y_pred.append(pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "spam=pd.read_csv(r'C:\\Users\\Armaan\\Desktop\\WORKSPACE\\Datasets\\Spam_Not_Spam_Dataset\\spam_or_not_spam.csv')\n",
    "x_train,x_test,y_train,y_test=train_test_split(spam.email,spam.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=fit(x_train,y_train)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred=predict(x_test,dictionary)\n",
    "#y_train_pred=predict(x_train,dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98       633\n",
      "           1       0.99      0.74      0.84       117\n",
      "\n",
      "    accuracy                           0.96       750\n",
      "   macro avg       0.97      0.87      0.91       750\n",
      "weighted avg       0.96      0.96      0.95       750\n",
      "\n",
      "[[632   1]\n",
      " [ 31  86]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(classification_report(y_test,y_test_pred))\n",
    "print(confusion_matrix(y_test,y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c19b36fe549fe2dce1ac32d0dd317d0a363043eb1c14a547f46436cb05190cdf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
